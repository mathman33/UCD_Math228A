\documentclass{article}


\usepackage[margin=0.6in]{geometry}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{physics}
\usepackage{enumerate}
\usepackage{array}
\usepackage{tikz}
\usepackage{pgfplots}
\newcommand{\Rl}{\mathbb{R}}
\newcommand{\sgn}{\text{sgn}}
\newcommand{\E}{\varepsilon}
\newcommand{\f}[3]{#1\ :\ #2 \rightarrow #3}

\DeclareMathOperator*{\esssup}{\text{ess~sup}}

\title{MAT 228A Notes}
\author{Sam Fleischer}
\date{October 04, 2016}

\begin{document}
    \maketitle

    \section{Using Finite Difference Methods to Solve Poisson Equations (1D)}
        Suppose $u_{xx} = f$, $u(0) = \alpha$, $u(1) = \beta$.  Let's discretize the domain ($[0,1]$) by choosing equally spaced points.  Let $x_j = jh$ where $h$ is the spacing between points.  So $0 = x_0$ and let $1 = x_{N+1}$ so that the interior points are $x_1,\dots, x_N$.  So $h = \frac{1}{N+1}$.

        Notation: use $u_j \colonapprox u(x_j)$.

        Next we replace the Laplacian $\frac{\partial^2}{\partial x^2}$ with a difference operator $D$.
        \begin{align}
            \frac{u_{j-1} - 2u_{j} + u_{j+1}}{h^2} = f_j \coloneqq f(x_j)
        \end{align}
        This is a linear algebra problem with $N$ unknowns.  Collect them into a vector $\underline{u}$:
        \begin{align}
            \underline{u} = \qty(\begin{array}{c}
                u_1 \\ \vdots \\ u_N
            \end{array})
        \end{align}
        The above equations are of the form $A\underline{u} = \underline{b}$.  What are $A$ and $\underline{b}$?
        \begin{align}
            \frac{1}{h^2}\qty(\begin{array}{ccccccc}
                -2 & 1 & 0 & 0 & \dots & 0 & 0 \\
                1 & -2 & 1 & 0 & \dots & 0 & 0 \\
                0 & 1 & -2 & 1 & \dots & 0 & 0 \\
                \vdots & \vdots & \ddots & \ddots & \ddots & \vdots & \vdots \\
                0 & 0 & \dots & 1 & -2 & 1 & 0 \\
                0 & 0 & \dots & 0 & 1 & -2 & 1 \\
                0 & 0 & \dots & 0 & 0 & 1 & -2
            \end{array})\qty(\begin{array}{c}
                u_1 \\ u_2 \\ \vdots \\ u_{N-1} \\ u_N
            \end{array}) = \qty(\begin{array}{c}
                f_1 - \frac{\alpha}{h^2} \\ f_2 \\ \vdots \\ f_{N-1} \\ f_N - \frac{\beta}{h^2}
            \end{array})
        \end{align}
        So $A$ is the above tri-diagonal matrix and $\underline{b}$ is
        \begin{align}
            \underline{b} = \qty(\begin{array}{c}
                f_1 - \frac{\alpha}{h^2} \\ f_2 \\ \vdots \\ f_{N-1} \\ f_N - \frac{\beta}{h^2}
            \end{array}).
        \end{align}
        What if
        \begin{align}
            \underline{u} = \qty(\begin{array}{c}
                u_0 \\ u_1 \\ \vdots \\ u_N \\ u_{N+1}
            \end{array})?
        \end{align}
        Then
        \begin{align}
            A = \frac{1}{h^2}\qty(\begin{array}{ccccccccc}
                h^2 & 0 & 0 & 0 & 0 & \dots & 0 & 0 \\
                1 & -2 & 1 & 0 & 0 & \dots & 0 & 0 \\
                0 & 1 & -2 & 1 & 0 & \dots & 0 & 0 \\
                0 & 0 & 1 & -2 & 1 & \dots & 0 & 0 \\
                \vdots & \vdots & \vdots & \ddots & \ddots & \ddots & \vdots & \vdots \\
                0 & 0 & 0 & \dots & 1 & -2 & 1 & 0 \\
                0 & 0 & 0 & \dots & 0 & 1 & -2 & 1 \\
                0 & 0 & 0 & \dots & 0 & 0 & 0 & h^2
            \end{array}), \qquad \underline{b} = \qty(\begin{array}{c}
                \alpha \\ f_1 \\ f_2 \\ \vdots \\ f_{N-1} \\ f_N \\ \beta
            \end{array})
        \end{align}
        \subsection{Errors}
            Anyway, how close is $u_j$ to $u(x_j)$ (which is the solution to the PDE)?  The error at a point is
            \begin{align}
                e_j^h = u_j^h - u(x_j)
            \end{align}
            which can be put into a vector
            \begin{align}
                \underline{e}^h = \underline{u}^h - \underline{u}_\text{sol}
            \end{align}
            where
            \begin{align}
                (\underline{u}_\text{sol})_j = u(x_j).
            \end{align}
            We would like
            \begin{align}
                \norm{\underline{e}^h} \rightarrow 0 \qquad \text{as} \qquad h \rightarrow 0
            \end{align}
            where $\norm{\cdot}$ is the appropriate norm.  If the error goes to $0$ as the mesh spacing $h$ goes to zero, the method (numerical scheme) is called a ``convergent scheme''.  What norm should we use to measure the errors?
        \subsection{Vector, Matrix, and Function norms}
            Let $\underline{x} \in \Rl^n$.  Then
            \begin{align}
                \norm{\underline{x}}_2^2 = \sum_{j=1}^n x_j^2 \qquad \qquad \norm{\underline{x}}_1 = \sum_{j=1}^n \abs{x_j} \qquad \qquad \norm{\underline{x}}_\infty = \max_{j=1,\dots,n}\abs{x_j}
            \end{align}
            Let $\f{u}{(a,b)}{\Rl}$.  Then
            \begin{align}
                \norm{u}_2^2 = \int_a^b \abs{u(x)}^2\dd x \qquad \qquad \norm{u}_1 = \int_a^b \abs{u(x)}\dd x \qquad \qquad \norm{u}_\infty = \esssup_{x\in(a,b)}\abs{u(x)}
            \end{align}
            Example: $u(x) = 1$ on $(0,1)$
            \begin{align}
                \norm{u}_2 = \norm{u}_1 = \norm{u}_\infty = 1
            \end{align}
            But if we sample on the mesh, where $(\underline{u})_j = u(x_j)$ is a vector of $1$'s, we have
            \begin{align}
                \norm{\underline{u}}_2 = \sqrt{N} \qquad \qquad \norm{\underline{u}}_1 = N \qquad \qquad \norm{\underline{u}}_\infty = 1
            \end{align}
            So we should discretize the function norms, rather than using vector norms on finite-dimensional spaces.  How?
            \subsubsection{Discretized Function norms}
                $\underline{e}_h$ is a grid function.  Define
                \begin{align}
                    \norm{\underline{e}_h}_2^2 = h\sum_{j=1}^N e_j^2
                \end{align}
                Similarly,
                \begin{align}
                    \norm{\underline{e}_h}_1 = h \sum_{j=1}^N\abs{e_j}
                \end{align}
                The norm is mesh-dependent.  The tricky thing here is the notation of the norm.  $\norm{\cdot}_2$ denotes the specific norm on the specific mesh space created.
            \subsubsection{Induced Matrix Norms}
                Let $A$ be some matrix, $\f{A}{\Rl^n}{\Rl^n}$ a linear operator on $\Rl^n$.  Then the definition of the operator norm is
                \begin{align}
                    \norm{A} \coloneqq \sup_{\norm{x} = 1} \norm{Ax}
                \end{align}
                Supposing $\Rl^n$ is equipped with $\norm{\cdot}_\infty$, then define
                \begin{align}
                    \norm{A}_\infty = \max_i \underbrace{\sum_{j=1}^n \abs{a_{ij}}}_{\text{sum over the $i$\textsuperscript{th} row}} \qquad \text{(max row sum)} \\
                \end{align}
                If $\Rl^n$ is equipped with $\norm{\cdot}_1$, then
                \begin{align}
                    \norm{A}_1 = \max_j \underbrace{\sum_{i=1}^n \abs{a_{ij}}}_{\text{sum over the $j$\textsuperscript{th} column}} \qquad \text{(max column sum)} \\
                \end{align}
                If $\Rl^n$ is equipped with $\norm{\cdot}_2$, then
                \begin{align}
                    \norm{A}_2 = \sqrt{\rho(A^*A)} \qquad \text{(largest singular value)}
                \end{align}
                where $\rho$ denotes the spectal radius and $A^*$ is the conjugate transpose of $A$.  The spectral radius is defined as the modulus of the largest eigenvalue.

                These are proved in Hunter-Nachtergaele.  For example, to prove $\norm{A}_\infty$ is the maximum row sum, first we prove boundedness of $\norm{\cdot}_\infty$.
                \begin{align}
                    \norm{A}_\infty = \max_{\norm{x}_\infty = 1} \norm{Ax}_\infty = \max_{\norm{x} = 1}\max_i\abs{\sum_{j=1}^n a_{ij}x_j} \leq \max_{\norm{x}=1}\max_i\sum_{j=1}^n\abs{a_{ij}}\abs{x_j} \leq \max_i\sum_{j=1}^n\abs{a_{ij}}
                \end{align}
                Then we acheive $\norm{\cdot}_\infty$. Let $I$ be the index of the row where $\norm{A}_\infty$ is maximized.  Then
                \begin{align}
                    \norm{A}_\infty = \sum_{j=1}^n\abs{a_{Ij}}
                \end{align}
                So picking $(x)_j = \sgn(a_{Ij})$ yields equality.

                In general, for any induced matrix norm,
                \begin{align}
                    \frac{\norm{Ax}}{\norm{x}} \leq \max_{x \neq 0}\frac{\norm{Ax}}{\norm{x}} \eqqcolon \norm{A}
                \end{align}
                So by the definition of the matrix norm, we have the inequality
                \begin{align}
                    \norm{Ax} \leq \norm{A}\norm{x}.
                \end{align}

\end{document}



















